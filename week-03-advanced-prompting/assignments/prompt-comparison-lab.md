# Assignment: Prompt Comparison Lab

## Goal
Explore the differences in LLM performance when using:
- Zero-shot prompting
- Few-shot prompting
- Chain-of-Thought (CoT) prompting

## Task
1. Choose at least 3 reasoning-based tasks (logic puzzles, math problems, or benchmark questions from the datasets provided).
2. Write three versions of the prompt:
   - Zero-shot: no examples
   - Few-shot: include 2–3 examples
   - CoT: explicitly ask the model to “think step by step”
3. Record outputs from each method.

## Deliverables
- A markdown or Jupyter notebook file containing:
  - Prompts used
  - Model outputs
  - Observations on accuracy and reasoning

## Reflection Questions
- Which method gave the most accurate results?  
- Did the CoT prompts improve reasoning quality?  
- How much do examples influence the model’s responses?  

