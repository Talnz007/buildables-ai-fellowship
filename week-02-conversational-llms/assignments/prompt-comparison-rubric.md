# Prompt Comparison Rubric

## Evaluation Framework for System Prompts

### Clarity (25 points)
- **Excellent (23-25)**: Prompt gives crystal clear instructions with specific examples
- **Good (18-22)**: Clear instructions with minor ambiguities
- **Fair (13-17)**: Generally clear but some confusion possible
- **Poor (0-12)**: Vague or confusing instructions

### Specificity (25 points)
- **Excellent (23-25)**: Highly specific with detailed behavioral guidelines
- **Good (18-22)**: Specific with good detail level
- **Fair (13-17)**: Somewhat specific but could be more detailed
- **Poor (0-12)**: Too generic or overly broad

### Consistency (25 points)
- **Excellent (23-25)**: Produces very consistent responses across multiple queries
- **Good (18-22)**: Generally consistent with minor variations
- **Fair (13-17)**: Some consistency but noticeable variations
- **Poor (0-12)**: Inconsistent or unpredictable responses

### Effectiveness (25 points)
- **Excellent (23-25)**: Achieves intended behavior perfectly
- **Good (18-22)**: Mostly achieves intended behavior
- **Fair (13-17)**: Partially effective
- **Poor (0-12)**: Doesn't achieve intended behavior

## Comparison Template
For each prompt, document:
1. **Intended Behavior**: What you wanted the AI to do
2. **Actual Behavior**: What it actually did
3. **Sample Responses**: 3-5 example responses to the same question
4. **Effectiveness Rating**: Score using rubric above
5. **Improvements**: How you would refine the prompt
