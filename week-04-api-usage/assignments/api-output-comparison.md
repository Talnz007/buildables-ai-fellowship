# Guideline: Comparing LLM API Outputs

When working with different LLMs like OpenAI's GPT series and Google's Gemini, you‚Äôll notice they produce different outputs even with the exact same prompt.  
This is due to differences in their training data, architecture, and fine-tuning.

Use the following rubric to critically evaluate and compare the outputs you receive during your assignment.

---

## üìù Comparison Rubric

For each task (Summarization, Q&A), evaluate the output from each model you test (if you have access to more than one) on the criteria below.

| Criteria | Description | Model A Score (1-5) | Model B Score (1-5) | Notes / Comments |
| :--- | :--- | :--- | :--- | :--- |
| **1. Relevance & Accuracy** | Does the output directly address the prompt? Is the information factually correct according to the source text? | | | |
| **2. Coherence & Readability** | Is the text well-written, grammatically correct, and easy to understand? Does it flow naturally? | | | |
| **3. Conciseness** | Does the model provide the information without unnecessary words or filler? Is the summary appropriately brief? | | | |
| **4. Instruction Following** | Did the model follow all constraints in the prompt (e.g., ‚Äúsummarize in three sentences,‚Äù ‚Äúanswer in a single paragraph‚Äù)? | | | |
| **5. Tone & Style** | Is the tone appropriate for the task (e.g., formal for a summary, conversational for a chatbot)? | | | |

---

## üìä Scoring Guide

- **1 - Poor**: Fails completely on the criterion. Irrelevant, inaccurate, or unreadable.  
- **2 - Fair**: Partially meets the criterion but has significant flaws.  
- **3 - Good**: Meets the criterion reasonably well but with minor issues.  
- **4 - Very Good**: Meets the criterion strongly with almost no issues.  
- **5 - Excellent**: Exceeds expectations; flawless for this criterion.  

---

## üí° Discussion Points

After filling out the rubric, reflect on these questions:

- Which model was better overall for summarization? Why?  
- Which model provided more accurate answers in the Q&A task?  
- Did one model seem more ‚Äúcreative‚Äù or more ‚Äúfactual‚Äù? How might this affect which one you‚Äôd choose for a specific application?  
- What differences did you notice in cost, speed, or token limits between the models?  

