# Guideline: Comparing LLM API Outputs

[cite_start]When working with different LLMs like OpenAI's GPT series and Google's Gemini, you'll notice they produce different outputs even with the exact same prompt[cite: 3]. This is due to differences in their training data, architecture, and fine-tuning.

Use this simple rubric to critically evaluate and compare the outputs you receive during your assignment.

## Comparison Rubric

For each task (Summarization, Q&A), evaluate the output from each model you test (if you have access to more than one) on the following criteria.

| Criteria | Description | Model A Score (1-5) | Model B Score (1-5) | Notes / Comments |
| :--- | :--- | :--- | :--- | :--- |
| **1. Relevance & Accuracy** | Does the output directly address the prompt? Is the information factually correct according to the source text? | | | |
| **2. Coherence & Readability** | Is the text well-written, grammatically correct, and easy to understand? Does it flow naturally? | | | |
| **3. Conciseness** | Does the model provide the information without unnecessary words or conversational filler? Is the summary appropriately brief? | | | |
| **4. Instruction Following** | Did the model adhere to all constraints in the prompt (e.g., "summarize in three sentences," "answer in a single paragraph")? | | | |
| **5. Tone & Style** | Is the tone of the response appropriate for the task (e.g., formal for a summary, conversational for a chatbot)? | | | |

---

### Scoring Guide

* **1 - Poor**: Fails completely on the criterion. The output is irrelevant, inaccurate, or unreadable.
* **2 - Fair**: Partially meets the criterion but has significant flaws.
* **3 - Good**: Meets the criterion reasonably well but has minor issues.
* **4 - Very Good**: Meets the criterion well with almost no issues.
* **5 - Excellent**: Exceeds expectations; the output is flawless for this criterion.

### Discussion Points

After filling out the rubric, consider these questions:
* Which model was better overall for summarization? Why?
* Which model provided more accurate answers in the Q&A task?
* Did one model seem more "creative" or "factual" than the other? How might this influence which one you'd choose for a specific application?
* [cite_start]What were the differences in cost or token limits between the models[cite: 3]?
