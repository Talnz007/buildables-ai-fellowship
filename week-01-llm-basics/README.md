# Week 1: Introduction to Large Language Models

## ğŸ¯ Learning Objectives
- Understand the fundamentals of Transformer architecture
- Learn about tokenization and its importance in LLMs
- Set up Python environment for LLM development
- Make your first API call to an LLM
- Explore different models (GPT,Gemini,Llama,Mistral...etc) and their characteristics

## ğŸ“‹ Weekly Schedule
- **Day 1-2**: Environment setup and theory
- **Day 3-4**: Hands-on coding with APIs
- **Day 5-6**: Project implementation
- **Day 7**: Project submission and peer review

## ğŸ›  Tools & Technologies
- Python 3.9+
- Gemini API
- Hugging Face Transformers
- Jupyter Notebooks (optional)

## ğŸ“š Required Readings
1. [Transformer Architecture Guide](./resources/readings/transformer_architecture.md)
2. [Tokenization Deep Dive](./resources/readings/tokenization_guide.md)

## ğŸ¬ Videos to Watch
See [videos.md](./resources/videos.md) for curated list

## ğŸ’» Sample Project
Check out our [Text Style Analyzer](./projects/sample-project/) for inspiration

## ğŸ“ Assignment
Complete the [Week 1 Assignment](./assignments/week01-assignment.md)

## ğŸ¤ Getting Help
- Join our Discord channel #week-01
- Attend office hours: Wednesday 7-8 PM
- Review [common issues](./solutions/common_issues.md)
